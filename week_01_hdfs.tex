\documentclass{article}
\usepackage[margin=0.75cm]{geometry}
 
\title{The Hadoop Distributed File System}
\begin{document}
\maketitle


Hadoop Distributed File System (HDFS) is a distributed file system designed to provide files with high bandwidth.
In the file-system, the files are only read, appended or deleted.
When a file is created, it is split into multiple blocks and the copies of those blocks one the nodes are called replicas.
Each block represents around 128MB and is replicated multiple times to provide higher bandwidth and better availability.

The HDFS architecture is composed of three major actors:
\begin{itemize}
	\item The Client
	\item The DataNode
	\item The NameNode
\end{itemize}

The Client is the library installed on the user side.
It offers the view of a typical file-system through which the user can create, delete or read, files or directories.
It communicates with the NameNode to get block locations and it communicates with DataNodes to get the actual data.

The DataNodes is a node that stores replicas.
It sends summary information to the NameNode about the replicas it holds.
It informs the NameNode when a replica is corrupted.
The DataNode receives orders from the NameNode, essentially about what the DataNode should do with their replicas.

The NameNode assumes the role of the file system supervisor.
It keeps in main memory the whole namespace (a.k.a the replicas and their locations, the blocks-to-file correspondence). 
It receives requests from the client about block locations and file operations.
It decides when and where a replica should be removed or copied.

The NameNode also keep track of the modifications done in the file-system by using a journal and it creates checkpoints.
Due to the NameNode importance, there are two variants of this node to prevent any node failure.
Both of them are active at the same time as the NameNode.
The CheckpointNode that downloads the current checkpoint of the NameNode and its journal in order to create a new checkpoint.
The BackupNode which synchronizes its namespace with the NameNode's namespace.
It can create new checkpoints without downloading the journal or the previous checkpoint, plus it can take over as a read only NameNode if the current NameNode fails.

The HDFS pipeline if fairly simple.
When a new file is created in HDFS, a new block is needed.
The NameNode chooses a series of DataNodes to host the replicas of this block and the client start uploading the data to the first DataNode.
When the replica is full or when the file is closed, the first DataNode will copy the replica to the next DataNode in the pipeline.
This last operation will repeat until the last DataNode of the pipeline gets a replica.
The client is notified when the pipeline is finished.
Note that, if the client continues to upload data, a new block is created by the NameNode alongside a new pipeline.

In order to get the highest bandwidth and a better fault tolerance system, HDFS follows a set of default policies.
Some of them are listed here:
\begin{itemize}
\item Keep at least three replicas per block to increase bandwidth and fault tolerance.
\item Replicas are placed on different parts of the clusters (nodes and racks).
\item The pipeline is designed to minimize bandwidth usages.
\end{itemize}

Note that those policies are configurable by the administrator.

Because HDFS does not considerate the memory usage of the DataNode, it may experience imbalance.
Such imbalance can also appear when new nodes enter the cluster.
Therefore, HDFS provides a load balance mechanism triggered when a threshold of imbalance is crossed.
\end{document}
